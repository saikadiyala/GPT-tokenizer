# GPT-tokenizer

# The tokenizer in this repository can train its vocabulary and merge tokens on a given text. It can encode and decode text.

# There are two main tokenizers in the repository: BasicTokenizer and RegexTokenizer. BasicTokenizer is the simplest and runs directly on text. 
# RegexTokenizer splits the input text by a regex pattern before tokenization. 
# This tokenizer is used to reproduce the tokenization of GPT-4. The GPT4Tokenizer is a wrapper around the RegexTokenizer.


